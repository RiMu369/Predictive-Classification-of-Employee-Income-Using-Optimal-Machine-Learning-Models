# -*- coding: utf-8 -*-
"""AI Internship EduNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1US2hQAu6-IYgnvGmHpt7ONLh4toy8nY-

# Installing and Importing Libraries
"""

!pip install streamlit pyngrok

# Employee salary prediction using adult.csv file
# Importing Libraries
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, OneHotEncoder, OrdinalEncoder, TargetEncoder
from sklearn.preprocessing import RobustScaler, MaxAbsScaler, Normalizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
import joblib
import streamlit as st
import os
import threading
import time
from pyngrok import ngrok
from sklearn.compose import ColumnTransformer

!ngrok authtoken 30H42VbebdfYJ2UzZ9BH9xrBf2f_6tWyvGuNRFyK92QpmaLR6

"""# Understanding the data"""

data_original = data = pd.read_csv("adult.csv")
data.head()

data.shape

data.tail()

data.isna()

data.isna().sum()

print(data.occupation.value_counts())

print(data.gender.value_counts())

print(data['marital-status'].value_counts())

print(data.education.value_counts())

print(data.workclass.value_counts())

print(data.age.value_counts())

"""# PreProcessing Data"""

data.occupation.replace({'?': 'Others'}, inplace = True)
data.occupation.value_counts()

data['native-country'].replace({'?': 'Others'}, inplace = True)
print(data['native-country'].value_counts())

data.workclass.replace({'?': 'Others'}, inplace = True)
data.workclass.value_counts()

data = data[data['workclass'] != 'Without-pay']
data = data[data['workclass'] != 'Never-worked']
data.workclass.value_counts()

data = data[data['education'] != 'Preschool']
data = data[data['education'] != '1st-4th']
data = data[data['education'] != '5th-6th']
data = data[data['education'] != '7th-8th']
data = data[data['education'] != '9th']
data = data[data['education'] != '10th']
data = data[data['education'] != '11th']
data = data[data['education'] != '12th']
data.education.value_counts()

data.drop(columns = ['education'], inplace = True)
data.head()

plt.boxplot(data['hours-per-week'])
plt.show()

data = data[(data['hours-per-week'] <= 55) & (data['hours-per-week'] >= 30)]
plt.boxplot(data['hours-per-week'])
plt.show()

plt.boxplot(data.age)
plt.show()

data = data[(data['age'] <= 75) & (data['age'] >= 17)]
plt.boxplot(data.age)
plt.show()

plt.boxplot(data['capital-gain'])
plt.show()

plt.boxplot(data['capital-loss'])
plt.show()

data.shape

Clean_Data = data

encoders = {
    "LabelEncoder" : LabelEncoder(),
    "OrdinalEncoder" : OrdinalEncoder(),
    "TargetEncoder" : TargetEncoder()
}
scalers = {
    "MinMaxScaler" : MinMaxScaler(),
    "StandardScaler" : StandardScaler(),
    "RobustScaler" : RobustScaler(),
    "MaxAbsScaler" : MaxAbsScaler()
}

data.columns

y = data['income']
x = data.drop(columns = ['income'])

x.head()

"""#Model Comparison

### Models
"""

models = {
    "LogisticRegression" : LogisticRegression(),
    "KNN" : KNeighborsClassifier(),
    "DecisionTree" : DecisionTreeClassifier(),
    "RandomForest" : RandomForestClassifier(),
    "GradientBoosting" : GradientBoostingClassifier(),
    "SVM" : SVC(),
    "MLPClassifier" : MLPClassifier()
}

"""### Running all combinations of encoding, scaler and model (Accuracy)"""

result = {}
for model_name, model in models.items():
  for encoder_name, encoder in encoders.items():
    for scaler_name, scaler in scalers.items():
      # Skip combinations of TargetEncoder as encoder and OneHotEncoder (which produces sparse data)
      if encoder_name == 'TargetEncoder' and 'OneHotEncoder' in encoders: # Check if OneHotEncoder is in the encoders dictionary
           print(f"Skipping combination of {encoder_name} and {scaler_name} as TargetEncoder does not support sparse input from OneHotEncoder.")
           continue


      x_encoded = x.copy()  # Create a copy to avoid modifying the original x
      categorical_cols = ['workclass', 'occupation', 'marital-status', 'gender', 'native-country', 'relationship', 'race']
      numerical_cols = [col for col in x_encoded.columns if col not in categorical_cols]

      # Apply the encoder to the entire dataset before splitting
      if encoder_name == 'OneHotEncoder':
          encoder_fit = OneHotEncoder(handle_unknown='ignore')
          x_encoded_categorical = encoder_fit.fit_transform(x_encoded[categorical_cols])

          # Combine numerical and encoded categorical features
          import numpy as np
          x_encoded = np.hstack((x_encoded[numerical_cols].values, x_encoded_categorical.toarray()))

          # If using OneHotEncoder, skip MinMaxScaler and Normalizer as they do not support sparse input.
          if scaler_name in ['MinMaxScaler', 'Normalizer']:
              print(f"Skipping {scaler_name} with OneHotEncoder as it does not support sparse input.")
              continue
          # If using OneHotEncoder and the scaler is StandardScaler, RobustScaler, or MaxAbsScaler, disable centering
          if scaler_name in ['StandardScaler', 'RobustScaler', 'MaxAbsScaler']:
              scaler.with_mean = False

      elif encoder_name == 'TargetEncoder':
          encoder_fit = TargetEncoder()
          # Apply TargetEncoder to the DataFrame of categorical columns
          x_encoded[categorical_cols] = encoder_fit.fit_transform(x_encoded[categorical_cols], y)

      else: # LabelEncoder and OrdinalEncoder
          for col in categorical_cols:
              if col in x_encoded.columns:
                  x_encoded[col] = encoder.fit_transform(x_encoded[col].values.reshape(-1, 1))


      xtrain, xtest, ytrain, ytest = train_test_split(x_encoded, y, test_size = 0.2, random_state = 42, stratify = y)

      # Convert to dense if the model is SVC and the data is sparse
      if model_name == 'SVM' and hasattr(xtrain, 'toarray'):
          xtrain = xtrain.toarray()
          xtest = xtest.toarray()


      # Fit the pipeline with the scaled and encoded training data
      pipe = Pipeline([("scaler", scaler), ("model", model)])
      pipe.fit(xtrain, ytrain)
      prediction = pipe.predict(xtest)
      acc = accuracy_score(ytest, prediction)
      result[f"{encoder_name}_{scaler_name}_{model_name}"] = acc
      print(f"{encoder_name}_{scaler_name}_{model_name} : {acc}")
      print(classification_report(ytest, prediction))

plt.figure(figsize=(25, 4))
plt.bar(result.keys(), result.values(), width = 0.5)
plt.xticks(rotation = 90)
plt.ylabel("Accuracy_Score")
plt.xlabel("Encoder_Scaler_Model")
plt.title("Model Comparison")
plt.show()

max_value = max(result.values())
max_keys = [key for key, value in result.items() if value == max_value]
print(max_keys, max_value, sep = "\n\n")

"""### Running all combinations of encoding, scaler for Gradient Boosting Classifier (Accuracy)"""

result = {}
model_name = "GradientBoosting"
model = GradientBoostingClassifier()
for encoder_name, encoder in encoders.items():
    for scaler_name, scaler in scalers.items():
      # Skip combinations of TargetEncoder as encoder and OneHotEncoder (which produces sparse data)
      if encoder_name == 'TargetEncoder' and 'OneHotEncoder' in encoders: # Check if OneHotEncoder is in the encoders dictionary
           print(f"Skipping combination of {encoder_name} and {scaler_name} as TargetEncoder does not support sparse input from OneHotEncoder.")
           continue


      x_encoded = x.copy()  # Create a copy to avoid modifying the original x
      categorical_cols = ['workclass', 'occupation', 'marital-status', 'gender', 'native-country', 'relationship', 'race']
      numerical_cols = [col for col in x_encoded.columns if col not in categorical_cols]

      # Apply the encoder to the entire dataset before splitting
      if encoder_name == 'OneHotEncoder':
          encoder_fit = OneHotEncoder(handle_unknown='ignore')
          x_encoded_categorical = encoder_fit.fit_transform(x_encoded[categorical_cols])

          # Combine numerical and encoded categorical features
          import numpy as np
          x_encoded = np.hstack((x_encoded[numerical_cols].values, x_encoded_categorical.toarray()))

          # If using OneHotEncoder, skip MinMaxScaler and Normalizer as they do not support sparse input.
          if scaler_name in ['MinMaxScaler', 'Normalizer']:
              print(f"Skipping {scaler_name} with OneHotEncoder as it does not support sparse input.")
              continue
          # If using OneHotEncoder and the scaler is StandardScaler, RobustScaler, or MaxAbsScaler, disable centering
          if scaler_name in ['StandardScaler', 'RobustScaler', 'MaxAbsScaler']:
              scaler.with_mean = False

      elif encoder_name == 'TargetEncoder':
          encoder_fit = TargetEncoder()
          # Apply TargetEncoder to the DataFrame of categorical columns
          x_encoded[categorical_cols] = encoder_fit.fit_transform(x_encoded[categorical_cols], y)

      else: # LabelEncoder and OrdinalEncoder
          for col in categorical_cols:
              if col in x_encoded.columns:
                  x_encoded[col] = encoder.fit_transform(x_encoded[col].values.reshape(-1, 1))


      xtrain, xtest, ytrain, ytest = train_test_split(x_encoded, y, test_size = 0.2, random_state = 42, stratify = y)

      # Convert to dense if the model is SVC and the data is sparse
      if model_name == 'SVM' and hasattr(xtrain, 'toarray'):
          xtrain = xtrain.toarray()
          xtest = xtest.toarray()


      # Fit the pipeline with the scaled and encoded training data
      pipe = Pipeline([("scaler", scaler), ("model", model)])
      pipe.fit(xtrain, ytrain)
      prediction = pipe.predict(xtest)
      acc = accuracy_score(ytest, prediction)
      result[f"{encoder_name}_{scaler_name}_{model_name}"] = acc
      print(f"{encoder_name}_{scaler_name}_{model_name} : {acc}")
      print(classification_report(ytest, prediction))

plt.figure(figsize=(25, 4))
plt.bar(result.keys(), result.values(), width = 0.5)
plt.xticks(rotation = 90)
plt.ylabel("Accuracy_Score")
plt.xlabel("Encoder_Scaler_Model")
plt.title("Model Comparison")
plt.show()

max_value_acc = max(result.values())
max_keys = best_acc = [key for key, value in result.items() if value == max_value_acc]
print(best_acc, max_value_acc, sep = "\n\n")

"""### Running all combinations of encoding, scaler and model (F1 Score)"""

result = {}
for model_name, model in models.items():
  for encoder_name, encoder in encoders.items():
    for scaler_name, scaler in scalers.items():
      # Skip combinations of TargetEncoder as encoder and OneHotEncoder (which produces sparse data)
      if encoder_name == 'TargetEncoder' and 'OneHotEncoder' in encoders: # Check if OneHotEncoder is in the encoders dictionary
           print(f"Skipping combination of {encoder_name} and {scaler_name} as TargetEncoder does not support sparse input from OneHotEncoder.")
           continue


      x_encoded = x.copy()  # Create a copy to avoid modifying the original x
      categorical_cols = ['workclass', 'occupation', 'marital-status', 'gender', 'native-country', 'relationship', 'race']
      numerical_cols = [col for col in x_encoded.columns if col not in categorical_cols]

      # Apply the encoder to the entire dataset before splitting
      if encoder_name == 'OneHotEncoder':
          encoder_fit = OneHotEncoder(handle_unknown='ignore')
          x_encoded_categorical = encoder_fit.fit_transform(x_encoded[categorical_cols])

          # Combine numerical and encoded categorical features
          import numpy as np
          x_encoded = np.hstack((x_encoded[numerical_cols].values, x_encoded_categorical.toarray()))

          # If using OneHotEncoder, skip MinMaxScaler and Normalizer as they do not support sparse input.
          if scaler_name in ['MinMaxScaler', 'Normalizer']:
              print(f"Skipping {scaler_name} with OneHotEncoder as it does not support sparse input.")
              continue
          # If using OneHotEncoder and the scaler is StandardScaler, RobustScaler, or MaxAbsScaler, disable centering
          if scaler_name in ['StandardScaler', 'RobustScaler', 'MaxAbsScaler']:
              scaler.with_mean = False

      elif encoder_name == 'TargetEncoder':
          encoder_fit = TargetEncoder()
          # Apply TargetEncoder to the DataFrame of categorical columns
          x_encoded[categorical_cols] = encoder_fit.fit_transform(x_encoded[categorical_cols], y)

      else: # LabelEncoder and OrdinalEncoder
          for col in categorical_cols:
              if col in x_encoded.columns:
                  x_encoded[col] = encoder.fit_transform(x_encoded[col].values.reshape(-1, 1))


      xtrain, xtest, ytrain, ytest = train_test_split(x_encoded, y, test_size = 0.2, random_state = 42, stratify = y)

      # Convert to dense if the model is SVC and the data is sparse
      if model_name == 'SVM' and hasattr(xtrain, 'toarray'):
          xtrain = xtrain.toarray()
          xtest = xtest.toarray()


      # Fit the pipeline with the scaled and encoded training data
      pipe = Pipeline([("scaler", scaler), ("model", model)])
      pipe.fit(xtrain, ytrain)
      prediction = pipe.predict(xtest)
      acc = f1_score(ytest, prediction, pos_label=">50K")
      result[f"{encoder_name}_{scaler_name}_{model_name}"] = acc
      print(f"{encoder_name}_{scaler_name}_{model_name} : {acc}")
      print(classification_report(ytest, prediction))

plt.figure(figsize=(25, 4))
plt.bar(result.keys(), result.values(), width = 0.5)
plt.xticks(rotation = 90)
plt.ylabel("Accuracy_Score")
plt.xlabel("Encoder_Scaler_Model")
plt.title("Model Comparison")
plt.show()

max_value = max(result.values())
max_keys = [key for key, value in result.items() if value == max_value]
print(max_keys, max_value, sep = "\n\n")

"""### Running all combinations of encoding, scaler for Gradient Boosting Classifier (F1 Score)"""

result = {}
model_name = "GradientBoosting"
model = GradientBoostingClassifier()
for encoder_name, encoder in encoders.items():
    for scaler_name, scaler in scalers.items():
      # Skip combinations of TargetEncoder as encoder and OneHotEncoder (which produces sparse data)
      if encoder_name == 'TargetEncoder' and 'OneHotEncoder' in encoders: # Check if OneHotEncoder is in the encoders dictionary
           print(f"Skipping combination of {encoder_name} and {scaler_name} as TargetEncoder does not support sparse input from OneHotEncoder.")
           continue


      x_encoded = x.copy()  # Create a copy to avoid modifying the original x
      categorical_cols = ['workclass', 'occupation', 'marital-status', 'gender', 'native-country', 'relationship', 'race']
      numerical_cols = [col for col in x_encoded.columns if col not in categorical_cols]

      # Apply the encoder to the entire dataset before splitting
      if encoder_name == 'OneHotEncoder':
          encoder_fit = OneHotEncoder(handle_unknown='ignore')
          x_encoded_categorical = encoder_fit.fit_transform(x_encoded[categorical_cols])

          # Combine numerical and encoded categorical features
          import numpy as np
          x_encoded = np.hstack((x_encoded[numerical_cols].values, x_encoded_categorical.toarray()))

          # If using OneHotEncoder, skip MinMaxScaler and Normalizer as they do not support sparse input.
          if scaler_name in ['MinMaxScaler', 'Normalizer']:
              print(f"Skipping {scaler_name} with OneHotEncoder as it does not support sparse input.")
              continue
          # If using OneHotEncoder and the scaler is StandardScaler, RobustScaler, or MaxAbsScaler, disable centering
          if scaler_name in ['StandardScaler', 'RobustScaler', 'MaxAbsScaler']:
              scaler.with_mean = False

      elif encoder_name == 'TargetEncoder':
          encoder_fit = TargetEncoder()
          # Apply TargetEncoder to the DataFrame of categorical columns
          x_encoded[categorical_cols] = encoder_fit.fit_transform(x_encoded[categorical_cols], y)

      else: # LabelEncoder and OrdinalEncoder
          for col in categorical_cols:
              if col in x_encoded.columns:
                  x_encoded[col] = encoder.fit_transform(x_encoded[col].values.reshape(-1, 1))


      xtrain, xtest, ytrain, ytest = train_test_split(x_encoded, y, test_size = 0.2, random_state = 42, stratify = y)

      # Convert to dense if the model is SVC and the data is sparse
      if model_name == 'SVM' and hasattr(xtrain, 'toarray'):
          xtrain = xtrain.toarray()
          xtest = xtest.toarray()


      # Fit the pipeline with the scaled and encoded training data
      pipe = Pipeline([("scaler", scaler), ("model", model)])
      pipe.fit(xtrain, ytrain)
      prediction = pipe.predict(xtest)
      acc = f1_score(ytest, prediction, pos_label=">50K")
      result[f"{encoder_name}_{scaler_name}_{model_name}"] = acc
      print(f"{encoder_name}_{scaler_name}_{model_name} : {acc}")
      print(classification_report(ytest, prediction))

plt.figure(figsize=(17, 3))
plt.bar(result.keys(), result.values(), width = 0.5)
plt.xticks(rotation = 90)
plt.ylabel("f1 score")
plt.xlabel("Encoder_Scaler_Model")
plt.title("Model Comparison")
plt.show()

max_value_f1 = max(result.values())
max_keys = best_f1 = [key for key, value in result.items() if value == max_value_f1]
print(max_keys, max_value_f1, sep = "\n\n")

# Build pipeline
pipeline = Pipeline([
    ('target_encode', TargetEncoder()),  # This encodes ALL categorical cols by default
    ('model', GradientBoostingClassifier())
])

pipeline.fit(xtrain, ytrain)

joblib.dump(pipeline, "best_model.pkl")

"""# Application"""

encoder, scaler, model = best_f1[0].split('_')
model = models[model]
joblib.dump(model, 'best_model.pkl')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# 
# #Loading trained models
# model = joblib.load("best_model.pkl")
# 
# st.set_page_config(page_title = "EMPLOYEE SALARY PREDICTION", page_icon = "üè¢üíº", layout = "centered")
# st.title("Predict Employee Salary")
# st.markdown("Predict the salary of your employee based on their age, workclass, education level, marital-status, occupation, relationship, race, gender, hours-per-week, nationality")
# st.header("Enter employee details...")
# age = st.slider("Age", 18, 65, 30)
# education = st.selectbox("Education", ["HS-grad", "Some-college", "Bachelors", "Masters", "Assoc-voc", "Assoc-acdm", "Prof-school", "Doctorate"])
# workclass = st.selectbox("Workclass", ["Private", "Self-emp-not-inc", "Local-gov", "State-gov", "Self-emp-inc", "Federal-gov", "Others"])
# marital_status = st.selectbox("Marital Status", ["Married-civ-spouse", "Never-married", "Divorced", "Separated", "Widowed", "Married-spouse-absent", "Married-AF-spouse"])
# occupation = st.selectbox("Occupation", ["Prof-specialty", "Craft-repair", "Exec-managerial", "Adm-clerical", "Sales", "Other-service", "Machine-op-inspct", "Others", "Transport-moving", "Handlers-cleaners", "Farming-fishing", "Tech-support", "Protective-serv", "Priv-house-serv", "Armed-Forces"])
# relationship = st.selectbox("Relationship", ["Husband", "Not-in-family", "Wife", "Own-child", "Unmarried", "Other-relative"])
# race = st.selectbox("Race", ["White", "Black", "Asian-Pac-Islander", "Amer-Indian-Eskimo", "Other"])
# gender = st.selectbox("Gender", ["Male", "Female"])
# hours_per_week = st.slider("Hours Per Week", 30, 55, 48)
# nationality = st.selectbox("Nationality", ['United-States', 'Others', 'Mexico', 'Philippines', 'Germany', 'Canada', 'Puerto-Rico', 'India', 'England', 'China', 'Japan', 'Cuba', 'Poland', 'Italy', 'Jamaica', 'Dominican-Republic', 'South', 'El-Salvador', 'Taiwan', 'Haiti', 'Vietnam', 'Portugal', 'Peru', 'Columbia', 'Nicaragua', 'Iran', 'Greece', 'Thailand', 'Cambodia', 'Trinadad&Tobago', 'Hong', 'Ireland', 'Outlying-US(Guam-USVI-etc)', 'Ecuador',	'Scotland',	'Guatemala', 'Honduras', 'Hungary', 'Yugoslavia', 'France', 'Laos'])
# capital_gain = st.slider("Capital-gain", 0, 100000, 50000)
# capital_loss = st.slider("Capital-loss", 0, 4000, 2000)
# 
# if education == "HS-grad":
#   edu_num = 9
# elif education == "Some-college":
#   edu_num = 10
# elif education == "Bachelors":
#   edu_num = 13
# elif education == "Masters":
#   edu_num = 14
# elif education == "Assoc-voc":
#   edu_num = 11
# elif education == "Assoc-acdm":
#   edu_num = 12
# elif education == "Prof-school":
#   edu_num = 15
# elif education == "Doctorate":
#   edu_num = 16
# 
# input_df = pd.DataFrame({"age": [age], "workclass": [workclass], "fnlwgt": [1], "educational-num": [edu_num], "marital-status": [marital_status], "occupation": [occupation], "relationship": [relationship], "race": [race], "gender": [gender], "capital-gain": [capital_gain], "capital-loss": [capital_loss], "hours-per-week": [hours_per_week], "native-country": [nationality]})
# 
# st.write("### Input Data")
# st.write(input_df)
# 
# if st.button("Predict Salary Class"):
#   prediction = model.predict(input_df)
#   st.success(f"Prediction : {prediction[0]}")
# 
# #Predict for a data set
# st.markdown("Predict For a Large Dataset")
# uploaded_file = st.file_uploader("Upload a CSV file", type=["csv"])
# if uploaded_file is not None:
#     data = pd.read_csv(uploaded_file)
#     st.write("Uploaded Data Preview: ", data.head())
#     if st.button("Predict Salary Class for all"):
#         prediction = model.predict(data)
#         data["Predicted"] = prediction
#         st.write("Predicted Data: ", data.head())
#     csv = data.to_csv(index=False).encode('utf-8')
#     st.download_button("Download Predicted Data", data=csv, file_name="predicted_data.csv", mime="text/csv")

def run_streamlit():
    os.system('streamlit run app.py --server.port 8501')

thread = threading.Thread(target = run_streamlit)
thread.start()

time.sleep(5)
public_url = ngrok.connect(8501)
print("Streamlit Application running at:", public_url)

